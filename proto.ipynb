{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc064f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training on Task 0\n",
      "======================================================================\n",
      "  Epoch 1/5 - Total Loss: 0.0488, CE Loss: 0.0488, EWC Loss: 0.0000\n",
      "  Epoch 2/5 - Total Loss: 0.0047, CE Loss: 0.0047, EWC Loss: 0.0000\n",
      "  Epoch 3/5 - Total Loss: 0.0025, CE Loss: 0.0025, EWC Loss: 0.0000\n",
      "  Epoch 4/5 - Total Loss: 0.0011, CE Loss: 0.0011, EWC Loss: 0.0000\n",
      "  Epoch 5/5 - Total Loss: 0.0024, CE Loss: 0.0024, EWC Loss: 0.0000\n",
      "\n",
      "Post-training accuracy on Task 0: 99.95%\n",
      "Computing Fisher Information for Task 0...\n",
      "\n",
      "==================================================\n",
      "Testing on all tasks:\n",
      "==================================================\n",
      "  Task 0: 99.95%\n",
      "\n",
      "  Average Accuracy: 99.95%\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "Training on Task 1\n",
      "======================================================================\n",
      "  Epoch 1/5 - Total Loss: 0.6381, CE Loss: 0.4644, EWC Loss: 0.0000\n",
      "  Epoch 2/5 - Total Loss: 0.0609, CE Loss: 0.0484, EWC Loss: 0.0000\n",
      "  Epoch 3/5 - Total Loss: 0.0403, CE Loss: 0.0316, EWC Loss: 0.0000\n",
      "  Epoch 4/5 - Total Loss: 0.0335, CE Loss: 0.0254, EWC Loss: 0.0000\n",
      "  Epoch 5/5 - Total Loss: 0.0217, CE Loss: 0.0161, EWC Loss: 0.0000\n",
      "\n",
      "Post-training accuracy on Task 1: 99.02%\n",
      "Computing Fisher Information for Task 1...\n",
      "\n",
      "==================================================\n",
      "Testing on all tasks:\n",
      "==================================================\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 99.02%\n",
      "\n",
      "  Average Accuracy: 49.51%\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "Training on Task 2\n",
      "======================================================================\n",
      "  Epoch 1/5 - Total Loss: 1.3652, CE Loss: 0.7563, EWC Loss: 0.0000\n",
      "  Epoch 2/5 - Total Loss: 0.0271, CE Loss: 0.0139, EWC Loss: 0.0000\n",
      "  Epoch 3/5 - Total Loss: 0.0202, CE Loss: 0.0089, EWC Loss: 0.0000\n",
      "  Epoch 4/5 - Total Loss: 0.0154, CE Loss: 0.0058, EWC Loss: 0.0000\n",
      "  Epoch 5/5 - Total Loss: 0.0117, CE Loss: 0.0030, EWC Loss: 0.0000\n",
      "\n",
      "Post-training accuracy on Task 2: 99.89%\n",
      "Computing Fisher Information for Task 2...\n",
      "\n",
      "==================================================\n",
      "Testing on all tasks:\n",
      "==================================================\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 5.04%\n",
      "  Task 2: 99.89%\n",
      "\n",
      "  Average Accuracy: 34.98%\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "Training on Task 3\n",
      "======================================================================\n",
      "  Epoch 1/5 - Total Loss: 1.2660, CE Loss: 0.6069, EWC Loss: 0.0000\n",
      "  Epoch 2/5 - Total Loss: 0.0241, CE Loss: 0.0049, EWC Loss: 0.0000\n",
      "  Epoch 3/5 - Total Loss: 0.0154, CE Loss: 0.0024, EWC Loss: 0.0000\n",
      "  Epoch 4/5 - Total Loss: 0.0124, CE Loss: 0.0014, EWC Loss: 0.0000\n",
      "  Epoch 5/5 - Total Loss: 0.0111, CE Loss: 0.0008, EWC Loss: 0.0000\n",
      "\n",
      "Post-training accuracy on Task 3: 99.85%\n",
      "Computing Fisher Information for Task 3...\n",
      "\n",
      "==================================================\n",
      "Testing on all tasks:\n",
      "==================================================\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 0.98%\n",
      "  Task 2: 0.96%\n",
      "  Task 3: 99.85%\n",
      "\n",
      "  Average Accuracy: 25.45%\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "Training on Task 4\n",
      "======================================================================\n",
      "  Epoch 1/5 - Total Loss: 1.7062, CE Loss: 0.8364, EWC Loss: 0.0000\n",
      "  Epoch 2/5 - Total Loss: 0.0850, CE Loss: 0.0459, EWC Loss: 0.0000\n",
      "  Epoch 3/5 - Total Loss: 0.0661, CE Loss: 0.0367, EWC Loss: 0.0000\n",
      "  Epoch 4/5 - Total Loss: 0.0563, CE Loss: 0.0301, EWC Loss: 0.0000\n",
      "  Epoch 5/5 - Total Loss: 0.0518, CE Loss: 0.0259, EWC Loss: 0.0000\n",
      "\n",
      "Post-training accuracy on Task 4: 99.24%\n",
      "Computing Fisher Information for Task 4...\n",
      "\n",
      "==================================================\n",
      "Testing on all tasks:\n",
      "==================================================\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 0.00%\n",
      "  Task 2: 0.00%\n",
      "  Task 3: 7.05%\n",
      "  Task 4: 99.24%\n",
      "\n",
      "  Average Accuracy: 21.26%\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "Accuracy matrix (rows=after task, cols=task performance):\n",
      "After Task 0: ['99.95']\n",
      "After Task 1: ['0.00', '99.02']\n",
      "After Task 2: ['0.00', '5.04', '99.89']\n",
      "After Task 3: ['0.00', '0.98', '0.96', '99.85']\n",
      "After Task 4: ['0.00', '0.00', '0.00', '7.05', '99.24']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "import copy\n",
    "\n",
    "\n",
    "def network_mnist(size_first_layer, size_second_layer):\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(784, size_first_layer)\n",
    "            self.fc2 = nn.Linear(size_first_layer, size_second_layer)\n",
    "            self.fc3 = nn.Linear(size_second_layer, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x  \n",
    "\n",
    "    return MLP()\n",
    "\n",
    "\n",
    "def ewc_train(model, task_number, epochs, criterion, optimizer, fisher_dict_prev, \n",
    "              parameter_dict_prev, ewc_lambda, device, train_stream):\n",
    "    experience = train_stream[task_number]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=64, shuffle=True)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_ewc_loss = 0\n",
    "        \n",
    "        for images, labels, *_ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            ce_loss = criterion(outputs, labels)\n",
    "            loss = ce_loss\n",
    "\n",
    "            # EWC regularization\n",
    "            ewc_loss = 0\n",
    "            if len(fisher_dict_prev) > 0:\n",
    "                for i in range(task_number):\n",
    "                    fisher_dict = fisher_dict_prev[i]\n",
    "                    optpar_dict = parameter_dict_prev[i]\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if name in fisher_dict:\n",
    "                            fisher = fisher_dict[name]\n",
    "                            optpar = optpar_dict[name]\n",
    "                            ewc_loss += (fisher * (param - optpar).pow(2)).sum()\n",
    "                \n",
    "                loss = ce_loss + (ewc_lambda / 2) * ewc_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_ce_loss += ce_loss.item()\n",
    "            total_ewc_loss += ewc_loss.item() if isinstance(ewc_loss, torch.Tensor) else ewc_loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_ce = total_ce_loss / len(train_loader)\n",
    "        avg_ewc = total_ewc_loss / len(train_loader)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs} - Total Loss: {avg_loss:.4f}, \"\n",
    "              f\"CE Loss: {avg_ce:.4f}, EWC Loss: {avg_ewc:.4f}\")\n",
    "\n",
    "\n",
    "def test_taskwise(model, task_number, device, test_stream):\n",
    "    experience = test_stream[task_number]\n",
    "    test_loader = DataLoader(experience.dataset, batch_size=64, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, *_ in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test(model, device, test_stream, num_tasks):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing on all tasks:\")\n",
    "    print(\"=\"*50)\n",
    "    sum_acc = 0\n",
    "    acc_list = []\n",
    "    for i in range(num_tasks):\n",
    "        acc = test_taskwise(model, i, device, test_stream)\n",
    "        print(f\"  Task {i}: {acc:.2f}%\")\n",
    "        sum_acc += acc\n",
    "        acc_list.append(acc)\n",
    "    avg_acc = sum_acc / num_tasks\n",
    "    print(f\"\\n  Average Accuracy: {avg_acc:.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "    return avg_acc, acc_list\n",
    "\n",
    "\n",
    "def compute_fisher_information(model, task_number, num_samples, device, train_stream):\n",
    "    \"\"\"\n",
    "    Compute diagonal Fisher Information Matrix using empirical Fisher.\n",
    "    Uses the squared gradients of the log-likelihood.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    experience = train_stream[task_number]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Initialize Fisher information dict\n",
    "    fisher_dict = {name: torch.zeros_like(param, device=device) \n",
    "                   for name, param in model.named_parameters() if param.requires_grad}\n",
    "\n",
    "    count = 0\n",
    "    for images, labels, *_ in train_loader:\n",
    "        if count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Use log probabilities for proper Fisher computation\n",
    "        log_probs = F.log_softmax(outputs, dim=1)\n",
    "        \n",
    "        # Select the log probability of the true class\n",
    "        loss = -log_probs[0, labels[0]]\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate squared gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                fisher_dict[name] += param.grad.detach().pow(2)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Average over samples\n",
    "    for name in fisher_dict:\n",
    "        fisher_dict[name] /= count\n",
    "\n",
    "    return fisher_dict\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize model\n",
    "    model = network_mnist(256, 128)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    epochs = 5\n",
    "    ewc_lambda = 500000 # Reduced from 10000, tune this based on results\n",
    "    \n",
    "    # Prepare benchmark\n",
    "    benchmark = SplitMNIST(n_experiences=5, seed=1, shuffle=False)\n",
    "    train_stream = benchmark.train_stream\n",
    "    test_stream = benchmark.test_stream\n",
    "    \n",
    "    # Storage for EWC\n",
    "    fisher_dict_prev = []\n",
    "    parameter_dict_prev = []\n",
    "    \n",
    "    # Track accuracies\n",
    "    all_accuracies = []\n",
    "    \n",
    "    for task in range(5):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training on Task {task}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Create fresh optimizer for each task\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train with EWC\n",
    "        ewc_train(model, task, epochs, criterion, optimizer, \n",
    "                  fisher_dict_prev, parameter_dict_prev, ewc_lambda, device, train_stream)\n",
    "        \n",
    "        # Test on current task\n",
    "        acc = test_taskwise(model, task, device, test_stream)\n",
    "        print(f\"\\nPost-training accuracy on Task {task}: {acc:.2f}%\")\n",
    "        \n",
    "        # Compute Fisher Information\n",
    "        print(f\"Computing Fisher Information for Task {task}...\")\n",
    "        fisher_dict = compute_fisher_information(model, task_number=task, \n",
    "                                                num_samples=500, device=device, \n",
    "                                                train_stream=train_stream)\n",
    "        fisher_dict_prev.append(fisher_dict)\n",
    "        \n",
    "        # Store optimal parameters (only trainable parameters)\n",
    "        original_weights = {name: param.clone().detach() \n",
    "                          for name, param in model.named_parameters() \n",
    "                          if param.requires_grad}\n",
    "        parameter_dict_prev.append(original_weights)\n",
    "        \n",
    "        # Test on all tasks seen so far\n",
    "        avg_acc, acc_list = test(model, device, test_stream, task + 1)\n",
    "        all_accuracies.append(acc_list[:task+1])\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nAccuracy matrix (rows=after task, cols=task performance):\")\n",
    "    for i, acc_list in enumerate(all_accuracies):\n",
    "        print(f\"After Task {i}: {[f'{a:.2f}' for a in acc_list]}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
