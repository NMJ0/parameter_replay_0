{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ae0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import network_mnist,naive_train,test_taskwise,test,benchmark,train_stream,test_stream,compute_fisher_information,apply_importance_mask,create_masked_weight_dict,load_non_zero_weights\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from plot import plot_parameter_importance\n",
    "import os\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c71226",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=network_mnist(256,128)\n",
    "model_2=network_mnist(256,128)\n",
    "#print(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_2.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_1_train(model,task_number, epochs,criterion,optimizer,device,weight_dicts):\n",
    "    experience = train_stream[task_number]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for images, labels, *_ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "       \n",
    "\n",
    "        #print(f\"Task {task_number}, Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e69e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training on Task 0\n",
      "======================================================================\n",
      "Accuracy on task 0: 99.95%\n",
      "Post-training accuracy on Task 0: 99.95%\n",
      "Accuracy on task 0: 99.95%\n",
      "Accuracy on task 0: 99.95%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 20.00%\n",
      "\n",
      "======================================================================\n",
      "Training on Task 1\n",
      "======================================================================\n",
      "Accuracy on task 0: 99.95%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 20.00%\n",
      "Accuracy on task 0: 99.95%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 20.00%\n",
      "Accuracy on task 0: 99.95%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 20.00%\n",
      "Accuracy on task 0: 99.95%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 20.00%\n",
      "Accuracy on task 1: 99.36%\n",
      "Post-training accuracy on Task 1: 99.36%\n",
      "Accuracy on task 1: 99.36%\n",
      "Accuracy on task 0: 0.85%\n",
      "Accuracy on task 1: 99.36%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 20.04%\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('figures', exist_ok=True)\n",
    "all_tasks_data = {}\n",
    "weight_dicts=[]\n",
    "for task in range(2):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training on Task {task}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    method_1_train(model, task, epochs, criterion=criterion, optimizer=optimizer, device=device, weight_dicts=weight_dicts)\n",
    "    acc = test_taskwise(model, task, device)\n",
    "    print(f\"Post-training accuracy on Task {task}: {acc:.2f}%\")\n",
    "\n",
    "    fisher_dict = compute_fisher_information(model, task_number=task, num_samples=500, \n",
    "                                             criterion=criterion, device=device)\n",
    "    percent_list = list(range(50,51))\n",
    "    accuracy_vs_percent = []\n",
    "    \n",
    "\n",
    "    original_weights = {name: param.clone() for name, param in model.state_dict().items()}\n",
    "    for p in percent_list:\n",
    "        model, mask_dict = apply_importance_mask(model, fisher_dict, importance_percent=p)\n",
    "        weight_dicts.append(create_masked_weight_dict(model, mask_dict))\n",
    "        acc_p = test_taskwise(model, task, device)\n",
    "        accuracy_vs_percent.append(acc_p)\n",
    "        model.load_state_dict(original_weights, strict=False)\n",
    "       \n",
    "    all_tasks_data[task] = (percent_list, accuracy_vs_percent)\n",
    "    test(model, device)\n",
    "    #plot_parameter_importance(percent_list, accuracy_vs_percent, task, save_path=f'figures/task_{task}_importance.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating Model on Task {i} after applying masks from all tasks\")\n",
    "    print(f\"{'='*70}\")\n",
    "   \n",
    "    print(f\"\\n-- Using mask from Task {i} --\")\n",
    "    model_2=copy.deepcopy(model)\n",
    "    masked_model = load_non_zero_weights(model_2, weight_dicts[i])\n",
    "    acc = test_taskwise(masked_model, i, device)\n",
    "    print(f\"Accuracy on Task {i} with mask from Task {i}: {acc:.2f}%\")\n",
    "    masked_model.load_state_dict(weight_dicts[i], strict=False)\n",
    "    acc = test_taskwise(masked_model, i, device)\n",
    "    print(f\"Accuracy on Task {i} with mask from Task {i} (direct load): {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbd6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training Split MNIST with EWC (Avalanche EWCPlugin Implementation)\n",
      "======================================================================\n",
      "EWC Lambda: 5000\n",
      "Learning Rate: 0.001\n",
      "Epochs per task: 5\n",
      "Batch size: 64\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Task 0 Training\n",
      "======================================================================\n",
      "  Epoch 1/5 - Loss: 0.0441 (CE: 0.0441, EWC: 0.0000), Acc: 99.36%\n",
      "  Epoch 2/5 - Loss: 0.0057 (CE: 0.0057, EWC: 0.0000), Acc: 99.88%\n",
      "  Epoch 3/5 - Loss: 0.0018 (CE: 0.0018, EWC: 0.0000), Acc: 99.94%\n",
      "  Epoch 4/5 - Loss: 0.0032 (CE: 0.0032, EWC: 0.0000), Acc: 99.91%\n",
      "  Epoch 5/5 - Loss: 0.0005 (CE: 0.0005, EWC: 0.0000), Acc: 99.99%\n",
      "\n",
      "Post-training accuracy:\n",
      "Accuracy on task 0: 99.95%\n",
      "\n",
      "Computing importances for Task 0...\n",
      "  Total importance: 0.002349\n",
      "  Average importance per param: 0.00000001\n",
      "\n",
      "======================================================================\n",
      "Testing on all tasks seen so far:\n",
      "======================================================================\n",
      "Accuracy on task 0: 99.95%\n",
      "Average accuracy: 99.95%\n",
      "\n",
      "======================================================================\n",
      "Task 1 Training\n",
      "======================================================================\n",
      "  Epoch 1/5 - Loss: 0.4571 (CE: 0.4553, EWC: 0.0000), Acc: 88.71%\n",
      "  Epoch 2/5 - Loss: 0.0515 (CE: 0.0497, EWC: 0.0000), Acc: 98.30%\n",
      "  Epoch 3/5 - Loss: 0.0307 (CE: 0.0291, EWC: 0.0000), Acc: 99.04%\n",
      "  Epoch 4/5 - Loss: 0.0194 (CE: 0.0179, EWC: 0.0000), Acc: 99.40%\n",
      "  Epoch 5/5 - Loss: 0.0154 (CE: 0.0139, EWC: 0.0000), Acc: 99.53%\n",
      "\n",
      "Post-training accuracy:\n",
      "Accuracy on task 1: 99.61%\n",
      "\n",
      "Computing importances for Task 1...\n",
      "  Total importance: 0.164995\n",
      "  Average importance per param: 0.00000070\n",
      "\n",
      "======================================================================\n",
      "Testing on all tasks seen so far:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 99.61%\n",
      "Average accuracy: 49.80%\n",
      "\n",
      "======================================================================\n",
      "Task 2 Training\n",
      "======================================================================\n",
      "  Epoch 1/5 - Loss: 0.6723 (CE: 0.5974, EWC: 0.0000), Acc: 87.18%\n",
      "  Epoch 2/5 - Loss: 0.0321 (CE: 0.0126, EWC: 0.0000), Acc: 99.64%\n",
      "  Epoch 3/5 - Loss: 0.0168 (CE: 0.0073, EWC: 0.0000), Acc: 99.81%\n",
      "  Epoch 4/5 - Loss: 0.0097 (CE: 0.0036, EWC: 0.0000), Acc: 99.89%\n",
      "  Epoch 5/5 - Loss: 0.0056 (CE: 0.0011, EWC: 0.0000), Acc: 99.99%\n",
      "\n",
      "Post-training accuracy:\n",
      "Accuracy on task 2: 99.95%\n",
      "\n",
      "Computing importances for Task 2...\n",
      "  Total importance: 0.008895\n",
      "  Average importance per param: 0.00000004\n",
      "\n",
      "======================================================================\n",
      "Testing on all tasks seen so far:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 2.89%\n",
      "Accuracy on task 2: 99.95%\n",
      "Average accuracy: 34.28%\n",
      "\n",
      "======================================================================\n",
      "Task 3 Training\n",
      "======================================================================\n",
      "  Epoch 1/5 - Loss: 0.8267 (CE: 0.7715, EWC: 0.0000), Acc: 89.43%\n",
      "  Epoch 2/5 - Loss: 0.0224 (CE: 0.0027, EWC: 0.0000), Acc: 99.96%\n",
      "  Epoch 3/5 - Loss: 0.0121 (CE: 0.0009, EWC: 0.0000), Acc: 99.98%\n",
      "  Epoch 4/5 - Loss: 0.0081 (CE: 0.0004, EWC: 0.0000), Acc: 99.99%\n",
      "  Epoch 5/5 - Loss: 0.0062 (CE: 0.0003, EWC: 0.0000), Acc: 100.00%\n",
      "\n",
      "Post-training accuracy:\n",
      "Accuracy on task 3: 99.90%\n",
      "\n",
      "Computing importances for Task 3...\n",
      "  Total importance: 0.000174\n",
      "  Average importance per param: 0.00000000\n",
      "\n",
      "======================================================================\n",
      "Testing on all tasks seen so far:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 12.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 99.90%\n",
      "Average accuracy: 27.99%\n",
      "\n",
      "======================================================================\n",
      "Task 4 Training\n",
      "======================================================================\n",
      "  Epoch 1/5 - Loss: 0.9011 (CE: 0.8509, EWC: 0.0000), Acc: 87.32%\n",
      "  Epoch 2/5 - Loss: 0.0486 (CE: 0.0313, EWC: 0.0000), Acc: 98.85%\n",
      "  Epoch 3/5 - Loss: 0.0297 (CE: 0.0183, EWC: 0.0000), Acc: 99.44%\n",
      "  Epoch 4/5 - Loss: 0.0231 (CE: 0.0140, EWC: 0.0000), Acc: 99.61%\n",
      "  Epoch 5/5 - Loss: 0.0190 (CE: 0.0106, EWC: 0.0000), Acc: 99.67%\n",
      "\n",
      "Post-training accuracy:\n",
      "Accuracy on task 4: 99.39%\n",
      "\n",
      "Computing importances for Task 4...\n",
      "  Total importance: 0.213799\n",
      "  Average importance per param: 0.00000091\n",
      "\n",
      "======================================================================\n",
      "Testing on all tasks seen so far:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 1.13%\n",
      "Accuracy on task 2: 0.11%\n",
      "Accuracy on task 3: 0.05%\n",
      "Accuracy on task 4: 99.39%\n",
      "Average accuracy: 20.14%\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS - All 5 Tasks\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 1.13%\n",
      "Accuracy on task 2: 0.11%\n",
      "Accuracy on task 3: 0.05%\n",
      "Accuracy on task 4: 99.39%\n",
      "Average Accuracy: 20.14%\n",
      "\n",
      "Task-wise Final Results:\n",
      "Task 0: 0.00%\n",
      "Task 1: 1.13%\n",
      "Task 2: 0.11%\n",
      "Task 3: 0.05%\n",
      "Task 4: 99.39%\n",
      "======================================================================\n",
      "\n",
      "Model saved to 'ewc_avalanche_plugin_model.pth'\n"
     ]
    }
   ],
   "source": [
    "from model import network_mnist, test_taskwise, test, train_stream, test_stream\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "# ==================== Avalanche-style EWC Implementation ====================\n",
    "\n",
    "def copy_params_dict(model):\n",
    "    \"\"\"Copy parameters from model (Avalanche utility function)\"\"\"\n",
    "    return [(name, param.data.clone()) for name, param in model.named_parameters()]\n",
    "\n",
    "def zerolike_params_dict(model):\n",
    "    \"\"\"Create zero-like parameter dictionary (Avalanche utility function)\"\"\"\n",
    "    return [(name, torch.zeros_like(param)) for name, param in model.named_parameters()]\n",
    "\n",
    "class EWCPlugin:\n",
    "    \"\"\"\n",
    "    Elastic Weight Consolidation (EWC) plugin adapted from Avalanche.\n",
    "    Exactly follows Avalanche's EWCPlugin implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ewc_lambda, mode='separate', decay_factor=None, keep_importance_data=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ewc_lambda: hyperparameter to weigh the penalty inside the total loss\n",
    "            mode: 'separate' to keep penalty for each previous experience,\n",
    "                  'online' to keep single penalty with decay\n",
    "            decay_factor: used only if mode is 'online'\n",
    "            keep_importance_data: if True, keep in memory parameter values and importances\n",
    "        \"\"\"\n",
    "        assert (decay_factor is None) or (mode == 'online'), \\\n",
    "            \"You need to set `online` mode to use `decay_factor`.\"\n",
    "        assert (decay_factor is not None) or (mode != 'online'), \\\n",
    "            \"You need to set `decay_factor` to use the `online` mode.\"\n",
    "        assert mode == 'separate' or mode == 'online', \\\n",
    "            'Mode must be separate or online.'\n",
    "\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.mode = mode\n",
    "        self.decay_factor = decay_factor\n",
    "\n",
    "        if self.mode == 'separate':\n",
    "            self.keep_importance_data = True\n",
    "        else:\n",
    "            self.keep_importance_data = keep_importance_data\n",
    "\n",
    "        self.saved_params = defaultdict(list)\n",
    "        self.importances = defaultdict(list)\n",
    "\n",
    "    def compute_ewc_penalty(self, model, exp_counter, device):\n",
    "        \"\"\"\n",
    "        Compute EWC penalty (equivalent to before_backward in Avalanche).\n",
    "        \"\"\"\n",
    "        if exp_counter == 0:\n",
    "            return torch.tensor(0).float().to(device)\n",
    "\n",
    "        penalty = torch.tensor(0).float().to(device)\n",
    "\n",
    "        if self.mode == 'separate':\n",
    "            for experience in range(exp_counter):\n",
    "                for (_, cur_param), (_, saved_param), (_, imp) in zip(\n",
    "                        model.named_parameters(),\n",
    "                        self.saved_params[experience],\n",
    "                        self.importances[experience]):\n",
    "                    penalty += (imp * (cur_param - saved_param).pow(2)).sum()\n",
    "        elif self.mode == 'online':\n",
    "            prev_exp = exp_counter - 1\n",
    "            for (_, cur_param), (_, saved_param), (_, imp) in zip(\n",
    "                    model.named_parameters(),\n",
    "                    self.saved_params[prev_exp],\n",
    "                    self.importances[prev_exp]):\n",
    "                penalty += (imp * (cur_param - saved_param).pow(2)).sum()\n",
    "        else:\n",
    "            raise ValueError('Wrong EWC mode.')\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def compute_importances(self, model, criterion, optimizer, dataset, device, batch_size):\n",
    "        \"\"\"\n",
    "        Compute EWC importance matrix for each parameter.\n",
    "        Exactly follows Avalanche's compute_importances method.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        # Set RNN-like modules on GPU to training mode to avoid CUDA error\n",
    "        if device.type == 'cuda':\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, torch.nn.RNNBase):\n",
    "                    warnings.warn(\n",
    "                        'RNN-like modules do not support '\n",
    "                        'backward calls while in `eval` mode on CUDA '\n",
    "                        'devices. Setting all `RNNBase` modules to '\n",
    "                        '`train` mode. May produce inconsistent '\n",
    "                        'output if such modules have `dropout` > 0.'\n",
    "                    )\n",
    "                    module.train()\n",
    "\n",
    "        # Initialize importances as list of tuples\n",
    "        importances = zerolike_params_dict(model)\n",
    "        \n",
    "        # Move importances to device\n",
    "        importances = [(name, imp.to(device)) for name, imp in importances]\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Handle Avalanche batch format: (x, y, task_labels, ...)\n",
    "            x, y = batch[0], batch[1]\n",
    "            # For Split MNIST, we don't need task_labels for forward pass\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)  # Direct forward pass (no avalanche_forward needed)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            for (k1, p), (k2, imp) in zip(model.named_parameters(), importances):\n",
    "                assert (k1 == k2)\n",
    "                if p.grad is not None:\n",
    "                    imp += p.grad.data.clone().pow(2)\n",
    "\n",
    "        # Average over mini batch length\n",
    "        for _, imp in importances:\n",
    "            imp /= float(len(dataloader))\n",
    "\n",
    "        # Print statistics\n",
    "        total_importance = sum(imp.sum().item() for _, imp in importances)\n",
    "        num_params = sum(imp.numel() for _, imp in importances)\n",
    "        print(f\"  Total importance: {total_importance:.6f}\")\n",
    "        print(f\"  Average importance per param: {total_importance/num_params:.8f}\")\n",
    "\n",
    "        return importances\n",
    "\n",
    "    def update_importances(self, importances, t):\n",
    "        \"\"\"\n",
    "        Update importance for each parameter based on currently computed importances.\n",
    "        Exactly follows Avalanche's update_importances method.\n",
    "        \"\"\"\n",
    "        if self.mode == 'separate' or t == 0:\n",
    "            self.importances[t] = importances\n",
    "        elif self.mode == 'online':\n",
    "            self.importances[t] = []\n",
    "            for (k1, old_imp), (k2, curr_imp) in \\\n",
    "                    zip(self.importances[t - 1], importances):\n",
    "                assert k1 == k2, 'Error in importance computation.'\n",
    "                self.importances[t].append(\n",
    "                    (k1, (self.decay_factor * old_imp + curr_imp)))\n",
    "\n",
    "            # Clear previous parameter importances\n",
    "            if t > 0 and (not self.keep_importance_data):\n",
    "                del self.importances[t - 1]\n",
    "        else:\n",
    "            raise ValueError(\"Wrong EWC mode.\")\n",
    "\n",
    "    def after_training_exp(self, model, criterion, optimizer, experience, device, batch_size, exp_counter):\n",
    "        \"\"\"\n",
    "        Compute importances after each experience (equivalent to Avalanche's after_training_exp).\n",
    "        \"\"\"\n",
    "        importances = self.compute_importances(model, criterion, optimizer, \n",
    "                                             experience.dataset, device, batch_size)\n",
    "        self.update_importances(importances, exp_counter)\n",
    "        self.saved_params[exp_counter] = copy_params_dict(model)\n",
    "        \n",
    "        # Clear previous parameter values\n",
    "        if exp_counter > 0 and (not self.keep_importance_data):\n",
    "            del self.saved_params[exp_counter - 1]\n",
    "\n",
    "\n",
    "# ==================== Training Script ====================\n",
    "\n",
    "# Initialize model\n",
    "model = network_mnist(256, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameters\n",
    "ewc_lambda = 5000\n",
    "learning_rate = 0.001\n",
    "epochs_per_task = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize EWC Plugin\n",
    "ewc = EWCPlugin(ewc_lambda=ewc_lambda, mode='separate')\n",
    "\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Training Split MNIST with EWC (Avalanche EWCPlugin Implementation)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"EWC Lambda: {ewc_lambda}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Epochs per task: {epochs_per_task}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Main training loop\n",
    "for task in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Task {task} Training\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get experience for current task\n",
    "    experience = train_stream[task]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Create optimizer for this task\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs_per_task):\n",
    "        total_loss = 0\n",
    "        total_loss_ce = 0\n",
    "        total_loss_ewc = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for images, labels, *_ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute task loss\n",
    "            loss_ce = criterion(outputs, labels)\n",
    "            \n",
    "            # Compute EWC penalty (equivalent to before_backward)\n",
    "            ewc_penalty = ewc.compute_ewc_penalty(model, task, device)\n",
    "            \n",
    "            # Total loss (following Avalanche's formula exactly)\n",
    "            loss = loss_ce + ewc_lambda * ewc_penalty\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            total_loss_ce += loss_ce.item()\n",
    "            total_loss_ewc += ewc_penalty.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_loss_ce = total_loss_ce / len(train_loader)\n",
    "        avg_loss_ewc = total_loss_ewc / len(train_loader)\n",
    "        accuracy = 100 * correct / total_samples\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_task} - Loss: {avg_loss:.4f} \"\n",
    "              f\"(CE: {avg_loss_ce:.4f}, EWC: {avg_loss_ewc:.4f}), Acc: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Test on current task\n",
    "    print(f\"\\nPost-training accuracy:\")\n",
    "    acc_current = test_taskwise(model, task, device)\n",
    "    \n",
    "    # Compute importances after training (equivalent to after_training_exp)\n",
    "    print(f\"\\nComputing importances for Task {task}...\")\n",
    "    ewc.after_training_exp(model, criterion, optimizer, experience, device, batch_size, task)\n",
    "    \n",
    "    # Test on all tasks seen so far\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Testing on all tasks seen so far:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    task_accuracies = []\n",
    "    for t in range(task + 1):\n",
    "        acc = test_taskwise(model, t, device)\n",
    "        task_accuracies.append(acc)\n",
    "    \n",
    "    avg_acc = sum(task_accuracies) / len(task_accuracies)\n",
    "    print(f\"Average accuracy: {avg_acc:.2f}%\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS - All 5 Tasks\")\n",
    "print(\"=\"*70)\n",
    "final_acc, final_acc_list = test(model, device)\n",
    "\n",
    "# Display task-wise breakdown\n",
    "print(\"\\nTask-wise Final Results:\")\n",
    "for i, acc in enumerate(final_acc_list):\n",
    "    print(f\"Task {i}: {acc:.2f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save model and EWC data\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'ewc_lambda': ewc_lambda,\n",
    "    'saved_params': dict(ewc.saved_params),\n",
    "    'importances': dict(ewc.importances),\n",
    "    'final_accuracies': final_acc_list\n",
    "}, 'ewc_avalanche_plugin_model.pth')\n",
    "print(\"\\nModel saved to 'ewc_avalanche_plugin_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c7f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EWC Training with Debugging\n",
      "======================================================================\n",
      "EWC Lambda: 1,000,000\n",
      "Learning Rate: 0.001\n",
      "Epochs per task: 5\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TASK 0 TRAINING\n",
      "======================================================================\n",
      "  Epoch 1 - Loss: 0.0439 (CE: 0.0439, EWC: 0.000000), Acc: 99.31%\n",
      "  Epoch 2 - Loss: 0.0058 (CE: 0.0058, EWC: 0.000000), Acc: 99.85%\n",
      "  Epoch 3 - Loss: 0.0029 (CE: 0.0029, EWC: 0.000000), Acc: 99.92%\n",
      "  Epoch 4 - Loss: 0.0012 (CE: 0.0012, EWC: 0.000000), Acc: 99.96%\n",
      "  Epoch 5 - Loss: 0.0019 (CE: 0.0019, EWC: 0.000000), Acc: 99.93%\n",
      "\n",
      "  Post-training Test:\n",
      "Accuracy on task 0: 99.91%\n",
      "\n",
      "  Computing importances for Task 0...\n",
      "  Computing Fisher over 198 batches...\n",
      "  Fisher Statistics:\n",
      "    Total: 0.003849\n",
      "    Mean: 0.0000000164\n",
      "    Max: 0.0001002361\n",
      "    Min: 0.0000000000\n",
      "  ✓ Stored 6 parameter groups\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS ACCURACY:\n",
      "======================================================================\n",
      "Accuracy on task 0: 99.91%\n",
      "Average: 99.91%\n",
      "\n",
      "======================================================================\n",
      "TASK 1 TRAINING\n",
      "======================================================================\n",
      "  Epoch 1 - Loss: 0.5143 (CE: 0.4237, EWC: 0.000000), Acc: 90.36%\n",
      "  Epoch 2 - Loss: 0.0648 (CE: 0.0454, EWC: 0.000000), Acc: 98.38%\n",
      "  Epoch 3 - Loss: 0.0366 (CE: 0.0251, EWC: 0.000000), Acc: 99.13%\n",
      "  Epoch 4 - Loss: 0.0261 (CE: 0.0177, EWC: 0.000000), Acc: 99.47%\n",
      "  Epoch 5 - Loss: 0.0243 (CE: 0.0163, EWC: 0.000000), Acc: 99.41%\n",
      "\n",
      "  Post-training Test:\n",
      "Accuracy on task 1: 99.61%\n",
      "\n",
      "  Computing importances for Task 1...\n",
      "  Computing Fisher over 189 batches...\n",
      "  Fisher Statistics:\n",
      "    Total: 0.148982\n",
      "    Mean: 0.0000006336\n",
      "    Max: 0.0009035909\n",
      "    Min: 0.0000000000\n",
      "  ✓ Stored 6 parameter groups\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS ACCURACY:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 99.61%\n",
      "Average: 49.80%\n",
      "\n",
      "======================================================================\n",
      "TASK 2 TRAINING\n",
      "======================================================================\n",
      "  Epoch 1 - Loss: 1.0990 (CE: 0.6447, EWC: 0.000000), Acc: 88.08%\n",
      "  Epoch 2 - Loss: 0.0421 (CE: 0.0154, EWC: 0.000000), Acc: 99.53%\n",
      "  Epoch 3 - Loss: 0.0244 (CE: 0.0103, EWC: 0.000000), Acc: 99.72%\n",
      "  Epoch 4 - Loss: 0.0144 (CE: 0.0044, EWC: 0.000000), Acc: 99.87%\n",
      "  Epoch 5 - Loss: 0.0129 (CE: 0.0035, EWC: 0.000000), Acc: 99.88%\n",
      "\n",
      "  Post-training Test:\n",
      "Accuracy on task 2: 99.84%\n",
      "\n",
      "  Computing importances for Task 2...\n",
      "  Computing Fisher over 176 batches...\n",
      "  Fisher Statistics:\n",
      "    Total: 0.040091\n",
      "    Mean: 0.0000001705\n",
      "    Max: 0.0013873201\n",
      "    Min: 0.0000000000\n",
      "  ✓ Stored 6 parameter groups\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS ACCURACY:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 2.45%\n",
      "Accuracy on task 2: 99.84%\n",
      "Average: 34.10%\n",
      "\n",
      "======================================================================\n",
      "TASK 3 TRAINING\n",
      "======================================================================\n",
      "  Epoch 1 - Loss: 0.9647 (CE: 0.5312, EWC: 0.000000), Acc: 92.18%\n",
      "  Epoch 2 - Loss: 0.0246 (CE: 0.0037, EWC: 0.000000), Acc: 99.86%\n",
      "  Epoch 3 - Loss: 0.0140 (CE: 0.0015, EWC: 0.000000), Acc: 99.96%\n",
      "  Epoch 4 - Loss: 0.0110 (CE: 0.0005, EWC: 0.000000), Acc: 99.99%\n",
      "  Epoch 5 - Loss: 0.0099 (CE: 0.0002, EWC: 0.000000), Acc: 100.00%\n",
      "\n",
      "  Post-training Test:\n",
      "Accuracy on task 3: 99.90%\n",
      "\n",
      "  Computing importances for Task 3...\n",
      "  Computing Fisher over 191 batches...\n",
      "  Fisher Statistics:\n",
      "    Total: 0.000084\n",
      "    Mean: 0.0000000004\n",
      "    Max: 0.0000069275\n",
      "    Min: 0.0000000000\n",
      "  ✓ Stored 6 parameter groups\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS ACCURACY:\n",
      "======================================================================\n",
      "Accuracy on task 0: 1.09%\n",
      "Accuracy on task 1: 1.71%\n",
      "Accuracy on task 2: 0.80%\n",
      "Accuracy on task 3: 99.90%\n",
      "Average: 25.88%\n",
      "\n",
      "======================================================================\n",
      "TASK 4 TRAINING\n",
      "======================================================================\n",
      "  Epoch 1 - Loss: 1.1024 (CE: 0.7322, EWC: 0.000000), Acc: 89.36%\n",
      "  Epoch 2 - Loss: 0.0970 (CE: 0.0477, EWC: 0.000000), Acc: 98.47%\n",
      "  Epoch 3 - Loss: 0.0672 (CE: 0.0346, EWC: 0.000000), Acc: 98.88%\n",
      "  Epoch 4 - Loss: 0.0593 (CE: 0.0297, EWC: 0.000000), Acc: 99.08%\n",
      "  Epoch 5 - Loss: 0.0503 (CE: 0.0239, EWC: 0.000000), Acc: 99.21%\n",
      "\n",
      "  Post-training Test:\n",
      "Accuracy on task 4: 98.94%\n",
      "\n",
      "  Computing importances for Task 4...\n",
      "  Computing Fisher over 185 batches...\n",
      "  Fisher Statistics:\n",
      "    Total: 2.749704\n",
      "    Mean: 0.0000116936\n",
      "    Max: 0.2504003942\n",
      "    Min: 0.0000000000\n",
      "  ✓ Stored 6 parameter groups\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS ACCURACY:\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.24%\n",
      "Accuracy on task 2: 0.05%\n",
      "Accuracy on task 3: 0.20%\n",
      "Accuracy on task 4: 98.94%\n",
      "Average: 19.89%\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.24%\n",
      "Accuracy on task 2: 0.05%\n",
      "Accuracy on task 3: 0.20%\n",
      "Accuracy on task 4: 98.94%\n",
      "Average Accuracy: 19.89%\n",
      "Task 0: 0.00%\n",
      "Task 1: 0.24%\n",
      "Task 2: 0.05%\n",
      "Task 3: 0.20%\n",
      "Task 4: 98.94%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from model import network_mnist, test_taskwise, test, train_stream, test_stream\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==================== DEBUG VERSION WITH FIXES ====================\n",
    "\n",
    "def copy_params_dict(model):\n",
    "    \"\"\"Copy parameters from model\"\"\"\n",
    "    return [(name, param.data.clone()) for name, param in model.named_parameters()]\n",
    "\n",
    "def zerolike_params_dict(model):\n",
    "    \"\"\"Create zero-like parameter dictionary\"\"\"\n",
    "    return [(name, torch.zeros_like(param)) for name, param in model.named_parameters()]\n",
    "\n",
    "class EWCPluginFixed:\n",
    "    \"\"\"\n",
    "    EWC Plugin with debugging and fixes for very small Fisher values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ewc_lambda, mode='separate'):\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.mode = mode\n",
    "        self.saved_params = defaultdict(list)\n",
    "        self.importances = defaultdict(list)\n",
    "        self.keep_importance_data = True\n",
    "\n",
    "    def compute_ewc_penalty(self, model, exp_counter, device):\n",
    "        \"\"\"Compute EWC penalty with debugging.\"\"\"\n",
    "        if exp_counter == 0:\n",
    "            return torch.tensor(0).float().to(device)\n",
    "\n",
    "        penalty = torch.tensor(0).float().to(device)\n",
    "\n",
    "        if self.mode == 'separate':\n",
    "            for experience in range(exp_counter):\n",
    "                exp_penalty = torch.tensor(0).float().to(device)\n",
    "                for (name_cur, cur_param), (name_saved, saved_param), (name_imp, imp) in zip(\n",
    "                        model.named_parameters(),\n",
    "                        self.saved_params[experience],\n",
    "                        self.importances[experience]):\n",
    "                    \n",
    "                    # Compute parameter-wise penalty\n",
    "                    param_penalty = (imp * (cur_param - saved_param).pow(2)).sum()\n",
    "                    exp_penalty += param_penalty\n",
    "                \n",
    "                penalty += exp_penalty\n",
    "                \n",
    "        return penalty\n",
    "\n",
    "    def compute_importances(self, model, criterion, optimizer, dataset, device, batch_size):\n",
    "        \"\"\"\n",
    "        Compute importances - FIXED VERSION.\n",
    "        The key issue: we need to use model in TRAIN mode to get meaningful gradients.\n",
    "        \"\"\"\n",
    "        # CRITICAL: Keep model in train mode to get proper gradients\n",
    "        model.train()\n",
    "        \n",
    "        # Initialize importances\n",
    "        importances = zerolike_params_dict(model)\n",
    "        importances = [(name, imp.to(device)) for name, imp in importances]\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f\"  Computing Fisher over {len(dataloader)} batches...\")\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            x, y = batch[0], batch[1]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Accumulate squared gradients\n",
    "            for (k1, p), (k2, imp) in zip(model.named_parameters(), importances):\n",
    "                assert k1 == k2\n",
    "                if p.grad is not None:\n",
    "                    imp += p.grad.data.clone().pow(2)\n",
    "\n",
    "        # Average over mini-batches\n",
    "        for name, imp in importances:\n",
    "            imp /= float(len(dataloader))\n",
    "\n",
    "        # Detailed statistics\n",
    "        total_importance = sum(imp.sum().item() for _, imp in importances)\n",
    "        num_params = sum(imp.numel() for _, imp in importances)\n",
    "        max_importance = max(imp.max().item() for _, imp in importances)\n",
    "        min_importance = min(imp.min().item() for _, imp in importances)\n",
    "        \n",
    "        print(f\"  Fisher Statistics:\")\n",
    "        print(f\"    Total: {total_importance:.6f}\")\n",
    "        print(f\"    Mean: {total_importance/num_params:.10f}\")\n",
    "        print(f\"    Max: {max_importance:.10f}\")\n",
    "        print(f\"    Min: {min_importance:.10f}\")\n",
    "\n",
    "        return importances\n",
    "\n",
    "    def after_training_exp(self, model, criterion, optimizer, experience, device, batch_size, exp_counter):\n",
    "        \"\"\"Compute and store importances after training.\"\"\"\n",
    "        print(f\"\\n  Computing importances for Task {exp_counter}...\")\n",
    "        \n",
    "        importances = self.compute_importances(\n",
    "            model, criterion, optimizer, \n",
    "            experience.dataset, device, batch_size\n",
    "        )\n",
    "        \n",
    "        # Store importances\n",
    "        self.importances[exp_counter] = importances\n",
    "        \n",
    "        # Store parameters\n",
    "        self.saved_params[exp_counter] = copy_params_dict(model)\n",
    "        \n",
    "        print(f\"  ✓ Stored {len(importances)} parameter groups\")\n",
    "\n",
    "\n",
    "# ==================== Training Script ====================\n",
    "\n",
    "model = network_mnist(256, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Try with MUCH higher lambda since Fisher values are tiny\n",
    "ewc_lambda = 1000000  # 1 million - compensate for tiny Fisher values\n",
    "learning_rate = 0.001\n",
    "epochs_per_task = 5\n",
    "batch_size = 64\n",
    "\n",
    "ewc = EWCPluginFixed(ewc_lambda=ewc_lambda, mode='separate')\n",
    "\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EWC Training with Debugging\")\n",
    "print(\"=\"*70)\n",
    "print(f\"EWC Lambda: {ewc_lambda:,}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Epochs per task: {epochs_per_task}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK {task} TRAINING\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    experience = train_stream[task]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs_per_task):\n",
    "        total_loss = 0\n",
    "        total_ce = 0\n",
    "        total_ewc = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (images, labels, *_) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss_ce = criterion(outputs, labels)\n",
    "            \n",
    "            # Compute EWC penalty\n",
    "            ewc_penalty = ewc.compute_ewc_penalty(model, task, device)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = loss_ce + ewc_lambda * ewc_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_ce += loss_ce.item()\n",
    "            total_ewc += ewc_penalty.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_ce = total_ce / len(train_loader)\n",
    "        avg_ewc = total_ewc / len(train_loader)\n",
    "        accuracy = 100 * correct / total_samples\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1} - Loss: {avg_loss:.4f} (CE: {avg_ce:.4f}, EWC: {avg_ewc:.6f}), Acc: {accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n  Post-training Test:\")\n",
    "    acc_current = test_taskwise(model, task, device)\n",
    "    \n",
    "    # Compute importances\n",
    "    ewc.after_training_exp(model, criterion, optimizer, experience, device, batch_size, task)\n",
    "    \n",
    "    # Test all tasks\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ALL TASKS ACCURACY:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    accuracies = []\n",
    "    for t in range(task + 1):\n",
    "        acc = test_taskwise(model, t, device)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    avg = sum(accuracies) / len(accuracies)\n",
    "    print(f\"Average: {avg:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "final_acc, final_list = test(model, device)\n",
    "\n",
    "for i, acc in enumerate(final_list):\n",
    "    print(f\"Task {i}: {acc:.2f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'ewc_lambda': ewc_lambda,\n",
    "    'saved_params': dict(ewc.saved_params),\n",
    "    'importances': dict(ewc.importances),\n",
    "}, 'ewc_debug_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b117e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EWC Lambda: 400,000\n",
      "\n",
      "======================================================================\n",
      "TASK 0\n",
      "======================================================================\n",
      "  Epoch 1: CE=1.7731, EWC=0.000000\n",
      "  Epoch 2: CE=0.4885, EWC=0.000000\n",
      "  Epoch 3: CE=0.1259, EWC=0.000000\n",
      "  Epoch 4: CE=0.0656, EWC=0.000000\n",
      "  Epoch 5: CE=0.0444, EWC=0.000000\n",
      "\n",
      "Accuracy on task 0: 99.76%\n",
      "  Computing Fisher...\n",
      "  Fisher sum: 0.100991\n",
      "\n",
      "  All tasks:\n",
      "Accuracy on task 0: 99.76%\n",
      "\n",
      "======================================================================\n",
      "TASK 1\n",
      "======================================================================\n",
      "  Epoch 1: CE=2.3428, EWC=0.000000\n",
      "  Epoch 2: CE=0.7495, EWC=0.000001\n",
      "  Epoch 3: CE=0.3871, EWC=0.000000\n",
      "  Epoch 4: CE=0.2735, EWC=0.000000\n",
      "  Epoch 5: CE=0.2214, EWC=0.000000\n",
      "\n",
      "Accuracy on task 1: 96.13%\n",
      "  Computing Fisher...\n",
      "  Fisher sum: 1.025975\n",
      "\n",
      "  All tasks:\n",
      "Accuracy on task 0: 38.44%\n",
      "Accuracy on task 1: 96.13%\n",
      "\n",
      "======================================================================\n",
      "TASK 2\n",
      "======================================================================\n",
      "  Epoch 1: CE=nan, EWC=nan\n",
      "  Epoch 2: CE=nan, EWC=nan\n",
      "  Epoch 3: CE=nan, EWC=nan\n",
      "  Epoch 4: CE=nan, EWC=nan\n",
      "  Epoch 5: CE=nan, EWC=nan\n",
      "\n",
      "Accuracy on task 2: 0.00%\n",
      "  Computing Fisher...\n",
      "  Fisher sum: nan\n",
      "\n",
      "  All tasks:\n",
      "Accuracy on task 0: 46.34%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "\n",
      "======================================================================\n",
      "TASK 3\n",
      "======================================================================\n",
      "  Epoch 1: CE=nan, EWC=nan\n",
      "  Epoch 2: CE=nan, EWC=nan\n",
      "  Epoch 3: CE=nan, EWC=nan\n",
      "  Epoch 4: CE=nan, EWC=nan\n",
      "  Epoch 5: CE=nan, EWC=nan\n",
      "\n",
      "Accuracy on task 3: 0.00%\n",
      "  Computing Fisher...\n",
      "  Fisher sum: nan\n",
      "\n",
      "  All tasks:\n",
      "Accuracy on task 0: 46.34%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "\n",
      "======================================================================\n",
      "TASK 4\n",
      "======================================================================\n",
      "  Epoch 1: CE=nan, EWC=nan\n",
      "  Epoch 2: CE=nan, EWC=nan\n",
      "  Epoch 3: CE=nan, EWC=nan\n",
      "  Epoch 4: CE=nan, EWC=nan\n",
      "  Epoch 5: CE=nan, EWC=nan\n",
      "\n",
      "Accuracy on task 4: 0.00%\n",
      "  Computing Fisher...\n",
      "  Fisher sum: nan\n",
      "\n",
      "  All tasks:\n",
      "Accuracy on task 0: 46.34%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "\n",
      "\n",
      "FINAL:\n",
      "Accuracy on task 0: 46.34%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 0.00%\n",
      "Average Accuracy: 9.27%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.267139479905437, [46.335697399527184, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import network_mnist, test_taskwise, test, train_stream, test_stream\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "class EWC_Final:\n",
    "    \"\"\"Final working version of EWC - simplified and fixed.\"\"\"\n",
    "    \n",
    "    def __init__(self, ewc_lambda):\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.saved_params = {}\n",
    "        self.importances = {}\n",
    "    \n",
    "    def penalty(self, model):\n",
    "        \"\"\"Compute EWC penalty.\"\"\"\n",
    "        if len(self.saved_params) == 0:\n",
    "            return 0\n",
    "        \n",
    "        loss = 0\n",
    "        for task_id in self.saved_params.keys():\n",
    "            for n, p in model.named_parameters():\n",
    "                _loss = self.importances[task_id][n] * (p - self.saved_params[task_id][n]) ** 2\n",
    "                loss += _loss.sum()\n",
    "        return loss\n",
    "    \n",
    "    def update(self, model, dataset, device, batch_size=64):\n",
    "        \"\"\"Compute and store Fisher information.\"\"\"\n",
    "        model.train()\n",
    "        \n",
    "        # Initialize Fisher\n",
    "        fisher = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            fisher[n] = torch.zeros_like(p)\n",
    "        \n",
    "        # Compute Fisher\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for x, y, *_ in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            for n, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.pow(2)\n",
    "        \n",
    "        # Average\n",
    "        for n in fisher:\n",
    "            fisher[n] /= len(dataloader)\n",
    "        \n",
    "        # Store\n",
    "        task_id = len(self.saved_params)\n",
    "        self.importances[task_id] = fisher\n",
    "        self.saved_params[task_id] = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        \n",
    "        total_fisher = sum(v.sum().item() for v in fisher.values())\n",
    "        print(f\"  Fisher sum: {total_fisher:.6f}\")\n",
    "\n",
    "\n",
    "# Initialize\n",
    "model = network_mnist(256, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "ewc_lambda = 400000\n",
    "lr = 0.001\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "ewc = EWC_Final(ewc_lambda)\n",
    "\n",
    "print(f\"EWC Lambda: {ewc_lambda:,}\\n\")\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TASK {task}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    experience = train_stream[task]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # Try SGD instead of Adam\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_ce, total_ewc = 0, 0, 0\n",
    "        \n",
    "        for x, y, *_ in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            ce_loss = criterion(out, y)\n",
    "            ewc_loss = ewc.penalty(model)\n",
    "            loss = ce_loss + ewc_lambda * ewc_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            total_ewc += ewc_loss.item() if isinstance(ewc_loss, torch.Tensor) else ewc_loss\n",
    "        \n",
    "        avg_ce = total_ce / len(train_loader)\n",
    "        avg_ewc = total_ewc / len(train_loader)\n",
    "        print(f\"  Epoch {epoch+1}: CE={avg_ce:.4f}, EWC={avg_ewc:.6f}\")\n",
    "    \n",
    "    # Test current\n",
    "    print()\n",
    "    test_taskwise(model, task, device)\n",
    "    \n",
    "    # Update EWC\n",
    "    print(\"  Computing Fisher...\")\n",
    "    ewc.update(model, experience.dataset, device, batch_size)\n",
    "    \n",
    "    # Test all\n",
    "    print(f\"\\n  All tasks:\")\n",
    "    for t in range(task+1):\n",
    "        test_taskwise(model, t, device)\n",
    "    print()\n",
    "\n",
    "print(\"\\nFINAL:\")\n",
    "test(model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206bfa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EWC Training - Corrected Lambda\n",
      "======================================================================\n",
      "EWC Lambda: 100 (Correct range: 1-5000)\n",
      "Learning Rate: 0.001\n",
      "Epochs: 5\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TASK 0\n",
      "======================================================================\n",
      "  E1/5 - Total: 0.0461 (CE: 0.0461, EWC: 0.000000), Acc: 99.27%\n",
      "  E2/5 - Total: 0.0044 (CE: 0.0044, EWC: 0.000000), Acc: 99.88%\n",
      "  E3/5 - Total: 0.0031 (CE: 0.0031, EWC: 0.000000), Acc: 99.91%\n",
      "  E4/5 - Total: 0.0030 (CE: 0.0030, EWC: 0.000000), Acc: 99.92%\n",
      "  E5/5 - Total: 0.0015 (CE: 0.0015, EWC: 0.000000), Acc: 99.94%\n",
      "\n",
      "  Task 0 Accuracy:\n",
      "Accuracy on task 0: 99.95%\n",
      "\n",
      "  Computing Fisher information for Task 0...\n",
      "    Computing Fisher over 198 batches...\n",
      "    Fisher - Total: 0.000855, Mean: 0.0000000036\n",
      "\n",
      "  All Tasks (Task 0 to 0):\n",
      "Accuracy on task 0: 99.95%\n",
      "  >>> Average: 99.95%\n",
      "\n",
      "======================================================================\n",
      "TASK 1\n",
      "======================================================================\n",
      "  E1/5 - Total: 0.4519 (CE: 0.4519, EWC: 0.000000), Acc: 87.67%\n",
      "  E2/5 - Total: 0.0516 (CE: 0.0516, EWC: 0.000000), Acc: 98.25%\n",
      "  E3/5 - Total: 0.0261 (CE: 0.0261, EWC: 0.000000), Acc: 99.20%\n",
      "  E4/5 - Total: 0.0175 (CE: 0.0175, EWC: 0.000000), Acc: 99.44%\n",
      "  E5/5 - Total: 0.0109 (CE: 0.0109, EWC: 0.000000), Acc: 99.58%\n",
      "\n",
      "  Task 1 Accuracy:\n",
      "Accuracy on task 1: 99.61%\n",
      "\n",
      "  Computing Fisher information for Task 1...\n",
      "    Computing Fisher over 189 batches...\n",
      "    Fisher - Total: 0.126875, Mean: 0.0000005396\n",
      "\n",
      "  All Tasks (Task 0 to 1):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 99.61%\n",
      "  >>> Average: 49.80%\n",
      "\n",
      "======================================================================\n",
      "TASK 2\n",
      "======================================================================\n",
      "  E1/5 - Total: 0.7612 (CE: 0.7577, EWC: 0.000035), Acc: 85.31%\n",
      "  E2/5 - Total: 0.0169 (CE: 0.0132, EWC: 0.000036), Acc: 99.54%\n",
      "  E3/5 - Total: 0.0101 (CE: 0.0069, EWC: 0.000032), Acc: 99.72%\n",
      "  E4/5 - Total: 0.0066 (CE: 0.0039, EWC: 0.000027), Acc: 99.90%\n",
      "  E5/5 - Total: 0.0045 (CE: 0.0022, EWC: 0.000023), Acc: 99.91%\n",
      "\n",
      "  Task 2 Accuracy:\n",
      "Accuracy on task 2: 99.95%\n",
      "\n",
      "  Computing Fisher information for Task 2...\n",
      "    Computing Fisher over 176 batches...\n",
      "    Fisher - Total: 0.009087, Mean: 0.0000000386\n",
      "\n",
      "  All Tasks (Task 0 to 2):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.10%\n",
      "Accuracy on task 2: 99.95%\n",
      "  >>> Average: 33.35%\n",
      "\n",
      "======================================================================\n",
      "TASK 3\n",
      "======================================================================\n",
      "  E1/5 - Total: 0.8389 (CE: 0.8347, EWC: 0.000042), Acc: 87.53%\n",
      "  E2/5 - Total: 0.0063 (CE: 0.0035, EWC: 0.000028), Acc: 99.89%\n",
      "  E3/5 - Total: 0.0045 (CE: 0.0023, EWC: 0.000022), Acc: 99.93%\n",
      "  E4/5 - Total: 0.0027 (CE: 0.0009, EWC: 0.000018), Acc: 99.98%\n",
      "  E5/5 - Total: 0.0018 (CE: 0.0002, EWC: 0.000015), Acc: 99.98%\n",
      "\n",
      "  Task 3 Accuracy:\n",
      "Accuracy on task 3: 99.90%\n",
      "\n",
      "  Computing Fisher information for Task 3...\n",
      "    Computing Fisher over 191 batches...\n",
      "    Fisher - Total: 0.000043, Mean: 0.0000000002\n",
      "\n",
      "  All Tasks (Task 0 to 3):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.64%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 99.90%\n",
      "  >>> Average: 25.13%\n",
      "\n",
      "======================================================================\n",
      "TASK 4\n",
      "======================================================================\n",
      "  E1/5 - Total: 1.0778 (CE: 1.0737, EWC: 0.000042), Acc: 83.38%\n",
      "  E2/5 - Total: 0.0360 (CE: 0.0323, EWC: 0.000037), Acc: 98.97%\n",
      "  E3/5 - Total: 0.0250 (CE: 0.0217, EWC: 0.000033), Acc: 99.29%\n",
      "  E4/5 - Total: 0.0148 (CE: 0.0118, EWC: 0.000030), Acc: 99.63%\n",
      "  E5/5 - Total: 0.0109 (CE: 0.0082, EWC: 0.000026), Acc: 99.75%\n",
      "\n",
      "  Task 4 Accuracy:\n",
      "Accuracy on task 4: 99.09%\n",
      "\n",
      "  Computing Fisher information for Task 4...\n",
      "    Computing Fisher over 185 batches...\n",
      "    Fisher - Total: 0.094012, Mean: 0.0000003998\n",
      "\n",
      "  All Tasks (Task 0 to 4):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 99.09%\n",
      "  >>> Average: 19.83%\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 99.09%\n",
      "Average Accuracy: 19.83%\n",
      "\n",
      "Task-wise breakdown:\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 0.05%\n",
      "  Task 2: 0.00%\n",
      "  Task 3: 0.00%\n",
      "  Task 4: 99.09%\n",
      "======================================================================\n",
      "✓ Model saved\n"
     ]
    }
   ],
   "source": [
    "from model import network_mnist, test_taskwise, test, train_stream, test_stream\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "class EWC_Correct:\n",
    "    \"\"\"Corrected EWC implementation with proper lambda scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self, ewc_lambda):\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.saved_params = {}\n",
    "        self.importances = {}\n",
    "    \n",
    "    def penalty(self, model):\n",
    "        \"\"\"Compute EWC penalty: Σ_t Σ_i F_t,i * (θ_i - θ*_t,i)^2\"\"\"\n",
    "        if len(self.saved_params) == 0:\n",
    "            return torch.tensor(0, device=next(model.parameters()).device, dtype=torch.float32)\n",
    "        \n",
    "        loss = torch.tensor(0, device=next(model.parameters()).device, dtype=torch.float32)\n",
    "        \n",
    "        for task_id in self.saved_params.keys():\n",
    "            for n, p in model.named_parameters():\n",
    "                fisher = self.importances[task_id][n]\n",
    "                saved_p = self.saved_params[task_id][n]\n",
    "                loss += (fisher * (p - saved_p).pow(2)).sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def update(self, model, criterion, dataset, device, batch_size=64):\n",
    "        \"\"\"Compute and store Fisher information after training on a task.\"\"\"\n",
    "        model.train()\n",
    "        \n",
    "        # Initialize Fisher\n",
    "        fisher = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            fisher[n] = torch.zeros_like(p, device=device)\n",
    "        \n",
    "        # Compute Fisher information\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f\"    Computing Fisher over {len(dataloader)} batches...\")\n",
    "        \n",
    "        for x, y, *_ in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Accumulate squared gradients (empirical Fisher)\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.pow(2)\n",
    "        \n",
    "        # Average over batches\n",
    "        for n in fisher:\n",
    "            fisher[n] /= len(dataloader)\n",
    "        \n",
    "        # Store for this task\n",
    "        task_id = len(self.saved_params)\n",
    "        self.importances[task_id] = fisher\n",
    "        self.saved_params[task_id] = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        \n",
    "        # Print statistics\n",
    "        total_fisher = sum(v.sum().item() for v in fisher.values())\n",
    "        mean_fisher = total_fisher / sum(v.numel() for v in fisher.values())\n",
    "        print(f\"    Fisher - Total: {total_fisher:.6f}, Mean: {mean_fisher:.10f}\")\n",
    "\n",
    "\n",
    "# ==================== Training Script ====================\n",
    "\n",
    "model = network_mnist(256, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# CORRECTED HYPERPARAMETERS\n",
    "ewc_lambda = 100  # Start with 100 (not 400,000!)\n",
    "learning_rate = 0.001\n",
    "epochs_per_task = 5\n",
    "batch_size = 64\n",
    "\n",
    "ewc = EWC_Correct(ewc_lambda)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EWC Training - Corrected Lambda\")\n",
    "print(\"=\"*70)\n",
    "print(f\"EWC Lambda: {ewc_lambda} (Correct range: 1-5000)\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Epochs: {epochs_per_task}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK {task}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    experience = train_stream[task]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    for epoch in range(epochs_per_task):\n",
    "        total_loss = 0\n",
    "        total_ce = 0\n",
    "        total_ewc_penalty = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for x, y, *_ in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(x)\n",
    "            ce_loss = criterion(out, y)\n",
    "            \n",
    "            # EWC penalty\n",
    "            ewc_penalty = ewc.penalty(model)\n",
    "            \n",
    "            # Total loss\n",
    "            total_task_loss = ce_loss + ewc_lambda * ewc_penalty\n",
    "            \n",
    "            total_task_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += total_task_loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            total_ewc_penalty += ewc_penalty.item() if isinstance(ewc_penalty, torch.Tensor) else ewc_penalty\n",
    "            \n",
    "            # Accuracy\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_ce = total_ce / len(train_loader)\n",
    "        avg_ewc = total_ewc_penalty / len(train_loader)\n",
    "        acc = 100 * correct / total_samples\n",
    "        \n",
    "        print(f\"  E{epoch+1}/5 - Total: {avg_loss:.4f} (CE: {avg_ce:.4f}, EWC: {avg_ewc:.6f}), Acc: {acc:.2f}%\")\n",
    "    \n",
    "    # Test current task\n",
    "    print(f\"\\n  Task {task} Accuracy:\")\n",
    "    test_taskwise(model, task, device)\n",
    "    \n",
    "    # Compute and store Fisher\n",
    "    print(f\"\\n  Computing Fisher information for Task {task}...\")\n",
    "    ewc.update(model, criterion, experience.dataset, device, batch_size)\n",
    "    \n",
    "    # Test all tasks seen so far\n",
    "    print(f\"\\n  All Tasks (Task 0 to {task}):\")\n",
    "    all_accs = []\n",
    "    for t in range(task + 1):\n",
    "        acc = test_taskwise(model, t, device)\n",
    "        all_accs.append(acc)\n",
    "    \n",
    "    avg_acc = sum(all_accs) / len(all_accs)\n",
    "    print(f\"  >>> Average: {avg_acc:.2f}%\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "final_avg, final_list = test(model, device)\n",
    "print(f\"\\nTask-wise breakdown:\")\n",
    "for i, acc in enumerate(final_list):\n",
    "    print(f\"  Task {i}: {acc:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'ewc_lambda': ewc_lambda,\n",
    "    'importances': ewc.importances,\n",
    "    'saved_params': ewc.saved_params,\n",
    "}, 'ewc_correct_model.pth')\n",
    "print(\"✓ Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9efd2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EWC Training - Fisher Computed During Training\n",
      "======================================================================\n",
      "Lambda: 400000\n",
      "Key: Fisher computed on EARLY TRAINING (large gradients)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TASK 0\n",
      "======================================================================\n",
      "\n",
      "  Step 1: Computing Fisher from training data...\n",
      "    Fisher - Total: 0.084578, Mean: 0.0000003597\n",
      "\n",
      "  Step 2: Training with EWC penalty...\n",
      "    E1 - Loss: 0.0435 (CE: 0.0435, EWC: 0.000000), Acc: 99.16%\n",
      "    E2 - Loss: 0.0030 (CE: 0.0030, EWC: 0.000000), Acc: 99.93%\n",
      "    E3 - Loss: 0.0015 (CE: 0.0015, EWC: 0.000000), Acc: 99.96%\n",
      "    E4 - Loss: 0.0017 (CE: 0.0017, EWC: 0.000000), Acc: 99.96%\n",
      "    E5 - Loss: 0.0015 (CE: 0.0015, EWC: 0.000000), Acc: 99.95%\n",
      "\n",
      "  Step 3: Evaluation\n",
      "    Task 0:\n",
      "Accuracy on task 0: 99.91%\n",
      "    All Tasks (0-0):\n",
      "Accuracy on task 0: 99.91%\n",
      "    Average: 99.91%\n",
      "\n",
      "======================================================================\n",
      "TASK 1\n",
      "======================================================================\n",
      "\n",
      "  Step 1: Computing Fisher from training data...\n",
      "    Fisher - Total: 10.848949, Mean: 0.0000461371\n",
      "\n",
      "  Step 2: Training with EWC penalty...\n",
      "    E1 - Loss: 0.7523 (CE: 0.5082, EWC: 0.000001), Acc: 85.87%\n",
      "    E2 - Loss: 0.1175 (CE: 0.0613, EWC: 0.000000), Acc: 98.01%\n",
      "    E3 - Loss: 0.0966 (CE: 0.0517, EWC: 0.000000), Acc: 98.30%\n",
      "    E4 - Loss: 0.0759 (CE: 0.0381, EWC: 0.000000), Acc: 98.73%\n",
      "    E5 - Loss: 0.0730 (CE: 0.0364, EWC: 0.000000), Acc: 98.68%\n",
      "\n",
      "  Step 3: Evaluation\n",
      "    Task 1:\n",
      "Accuracy on task 1: 99.07%\n",
      "    All Tasks (0-1):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 99.07%\n",
      "    Average: 49.53%\n",
      "\n",
      "======================================================================\n",
      "TASK 2\n",
      "======================================================================\n",
      "\n",
      "  Step 1: Computing Fisher from training data...\n",
      "    Fisher - Total: 15.286396, Mean: 0.0000650081\n",
      "\n",
      "  Step 2: Training with EWC penalty...\n",
      "    E1 - Loss: 0.9214 (CE: 0.4551, EWC: 0.000001), Acc: 90.15%\n",
      "    E2 - Loss: 0.0978 (CE: 0.0282, EWC: 0.000000), Acc: 99.33%\n",
      "    E3 - Loss: 0.0812 (CE: 0.0223, EWC: 0.000000), Acc: 99.40%\n",
      "    E4 - Loss: 0.0744 (CE: 0.0200, EWC: 0.000000), Acc: 99.50%\n",
      "    E5 - Loss: 0.0717 (CE: 0.0186, EWC: 0.000000), Acc: 99.49%\n",
      "\n",
      "  Step 3: Evaluation\n",
      "    Task 2:\n",
      "Accuracy on task 2: 99.31%\n",
      "    All Tasks (0-2):\n",
      "Accuracy on task 0: 0.28%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 99.31%\n",
      "    Average: 33.20%\n",
      "\n",
      "======================================================================\n",
      "TASK 3\n",
      "======================================================================\n",
      "\n",
      "  Step 1: Computing Fisher from training data...\n",
      "    Fisher - Total: 27.750113, Mean: 0.0001180123\n",
      "\n",
      "  Step 2: Training with EWC penalty...\n",
      "    E1 - Loss: 1.2462 (CE: 0.5572, EWC: 0.000002), Acc: 91.42%\n",
      "    E2 - Loss: 0.1129 (CE: 0.0136, EWC: 0.000000), Acc: 99.75%\n",
      "    E3 - Loss: 0.0873 (CE: 0.0092, EWC: 0.000000), Acc: 99.84%\n",
      "    E4 - Loss: 0.0777 (CE: 0.0076, EWC: 0.000000), Acc: 99.88%\n",
      "    E5 - Loss: 0.0732 (CE: 0.0067, EWC: 0.000000), Acc: 99.84%\n",
      "\n",
      "  Step 3: Evaluation\n",
      "    Task 3:\n",
      "Accuracy on task 3: 99.60%\n",
      "    All Tasks (0-3):\n",
      "Accuracy on task 0: 5.53%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.48%\n",
      "Accuracy on task 3: 99.60%\n",
      "    Average: 26.40%\n",
      "\n",
      "======================================================================\n",
      "TASK 4\n",
      "======================================================================\n",
      "\n",
      "  Step 1: Computing Fisher from training data...\n",
      "    Fisher - Total: 37.080658, Mean: 0.0001576921\n",
      "\n",
      "  Step 2: Training with EWC penalty...\n",
      "    E1 - Loss: 1.5898 (CE: 0.6161, EWC: 0.000002), Acc: 87.49%\n",
      "    E2 - Loss: 0.2237 (CE: 0.0755, EWC: 0.000000), Acc: 97.86%\n",
      "    E3 - Loss: 0.1774 (CE: 0.0596, EWC: 0.000000), Acc: 98.13%\n",
      "    E4 - Loss: 0.1591 (CE: 0.0523, EWC: 0.000000), Acc: 98.27%\n",
      "    E5 - Loss: 0.1492 (CE: 0.0494, EWC: 0.000000), Acc: 98.32%\n",
      "\n",
      "  Step 3: Evaluation\n",
      "    Task 4:\n",
      "Accuracy on task 4: 98.18%\n",
      "    All Tasks (0-4):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 98.18%\n",
      "    Average: 19.64%\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 98.18%\n",
      "Average Accuracy: 19.64%\n",
      "\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 0.00%\n",
      "  Task 2: 0.00%\n",
      "  Task 3: 0.00%\n",
      "  Task 4: 98.18%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from model import network_mnist, test_taskwise, test, train_stream, test_stream\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "class EWC_ProperFisher:\n",
    "    \"\"\"\n",
    "    EWC with PROPER Fisher computation.\n",
    "    Key insight: Compute Fisher DURING training (when gradients are large), \n",
    "    not after convergence (when gradients are tiny).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ewc_lambda):\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.saved_params = {}\n",
    "        self.importances = {}\n",
    "    \n",
    "    def penalty(self, model):\n",
    "        \"\"\"Compute EWC penalty.\"\"\"\n",
    "        if len(self.saved_params) == 0:\n",
    "            return torch.tensor(0, device=next(model.parameters()).device, dtype=torch.float32)\n",
    "        \n",
    "        loss = torch.tensor(0, device=next(model.parameters()).device, dtype=torch.float32)\n",
    "        \n",
    "        for task_id in self.saved_params.keys():\n",
    "            for n, p in model.named_parameters():\n",
    "                fisher = self.importances[task_id][n]\n",
    "                saved_p = self.saved_params[task_id][n]\n",
    "                loss += (fisher * (p - saved_p).pow(2)).sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_fisher_training(self, model, criterion, train_loader, device):\n",
    "        \"\"\"\n",
    "        Compute Fisher DURING early training.\n",
    "        This captures importance when gradients are large (not zero).\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        \n",
    "        fisher = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            fisher[n] = torch.zeros_like(p, device=device)\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        # Collect from ONE epoch of training\n",
    "        for x, y, *_ in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.pow(2)\n",
    "            \n",
    "            num_samples += x.size(0)\n",
    "        \n",
    "        # Normalize\n",
    "        for n in fisher:\n",
    "            fisher[n] /= float(num_samples)\n",
    "        \n",
    "        total_fisher = sum(v.sum().item() for v in fisher.values())\n",
    "        mean_fisher = total_fisher / sum(v.numel() for v in fisher.values())\n",
    "        print(f\"    Fisher - Total: {total_fisher:.6f}, Mean: {mean_fisher:.10f}\")\n",
    "        \n",
    "        return fisher\n",
    "    \n",
    "    def update_after_task(self, model, fisher_dict, task_id):\n",
    "        \"\"\"Store Fisher and parameters.\"\"\"\n",
    "        self.importances[task_id] = fisher_dict\n",
    "        self.saved_params[task_id] = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "\n",
    "\n",
    "# ==================== Training ====================\n",
    "\n",
    "model = network_mnist(256, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "ewc_lambda = 400000  # Tuned for MNIST network\n",
    "learning_rate = 0.001\n",
    "epochs_per_task = 5\n",
    "batch_size = 64\n",
    "\n",
    "ewc = EWC_ProperFisher(ewc_lambda)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EWC Training - Fisher Computed During Training\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Lambda: {ewc_lambda}\")\n",
    "print(f\"Key: Fisher computed on EARLY TRAINING (large gradients)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK {task}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    experience = train_stream[task]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # STEP 1: Compute Fisher from early training data (BEFORE convergence!)\n",
    "    print(f\"\\n  Step 1: Computing Fisher from training data...\")\n",
    "    fisher_early = ewc.compute_fisher_training(model, criterion, train_loader, device)\n",
    "    \n",
    "    # STEP 2: Train with EWC penalty\n",
    "    print(f\"\\n  Step 2: Training with EWC penalty...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs_per_task):\n",
    "        total_loss = 0\n",
    "        total_ce = 0\n",
    "        total_ewc = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for x, y, *_ in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            ce_loss = criterion(out, y)\n",
    "            ewc_penalty = ewc.penalty(model)\n",
    "            loss = ce_loss + ewc_lambda * ewc_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            total_ewc += ewc_penalty.item()\n",
    "            \n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_ce = total_ce / len(train_loader)\n",
    "        avg_ewc = total_ewc / len(train_loader)\n",
    "        acc = 100 * correct / total_samples\n",
    "        \n",
    "        print(f\"    E{epoch+1} - Loss: {avg_loss:.4f} (CE: {avg_ce:.4f}, EWC: {avg_ewc:.6f}), Acc: {acc:.2f}%\")\n",
    "    \n",
    "    # STEP 3: Test and store\n",
    "    print(f\"\\n  Step 3: Evaluation\")\n",
    "    print(f\"    Task {task}:\")\n",
    "    test_taskwise(model, task, device)\n",
    "    \n",
    "    ewc.update_after_task(model, fisher_early, task)\n",
    "    \n",
    "    print(f\"    All Tasks (0-{task}):\")\n",
    "    all_accs = []\n",
    "    for t in range(task + 1):\n",
    "        acc = test_taskwise(model, t, device)\n",
    "        all_accs.append(acc)\n",
    "    \n",
    "    print(f\"    Average: {sum(all_accs)/len(all_accs):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "final_avg, final_list = test(model, device)\n",
    "print()\n",
    "for i, acc in enumerate(final_list):\n",
    "    print(f\"  Task {i}: {acc:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "torch.save({'model': model.state_dict(), 'ewc': ewc}, 'ewc_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344407ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EWC Training - Final Working Version\n",
      "======================================================================\n",
      "Lambda: 50\n",
      "Key changes:\n",
      "  1. Store params AFTER training (not before)\n",
      "  2. Compute Fisher on all training data (not just first epoch)\n",
      "  3. Use model.eval() for Fisher (stable gradients)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TASK 0\n",
      "======================================================================\n",
      "\n",
      "  Training...\n",
      "    E1/5 - Loss: 0.0519 (CE: 0.0519, EWC: 0.000000), Acc: 99.13%\n",
      "    E2/5 - Loss: 0.0031 (CE: 0.0031, EWC: 0.000000), Acc: 99.92%\n",
      "    E3/5 - Loss: 0.0032 (CE: 0.0032, EWC: 0.000000), Acc: 99.89%\n",
      "    E4/5 - Loss: 0.0015 (CE: 0.0015, EWC: 0.000000), Acc: 99.94%\n",
      "    E5/5 - Loss: 0.0012 (CE: 0.0012, EWC: 0.000000), Acc: 99.96%\n",
      "\n",
      "  Task 0 Accuracy:\n",
      "Accuracy on task 0: 99.91%\n",
      "\n",
      "  Computing Fisher information...\n",
      "    Fisher stats - Total: 0.000192, Norm: 0.0000000008\n",
      "\n",
      "  All Tasks (0-0):\n",
      "Accuracy on task 0: 99.91%\n",
      "  Average: 99.91%\n",
      "\n",
      "======================================================================\n",
      "TASK 1\n",
      "======================================================================\n",
      "\n",
      "  Training...\n",
      "    E1/5 - Loss: 0.4759 (CE: 0.4759, EWC: 0.000000), Acc: 88.90%\n",
      "    E2/5 - Loss: 0.0400 (CE: 0.0400, EWC: 0.000000), Acc: 98.61%\n",
      "    E3/5 - Loss: 0.0237 (CE: 0.0237, EWC: 0.000000), Acc: 99.19%\n",
      "    E4/5 - Loss: 0.0150 (CE: 0.0150, EWC: 0.000000), Acc: 99.48%\n",
      "    E5/5 - Loss: 0.0128 (CE: 0.0128, EWC: 0.000000), Acc: 99.55%\n",
      "\n",
      "  Task 1 Accuracy:\n",
      "Accuracy on task 1: 98.68%\n",
      "\n",
      "  Computing Fisher information...\n",
      "    Fisher stats - Total: 0.011704, Norm: 0.0000000498\n",
      "\n",
      "  All Tasks (0-1):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 98.68%\n",
      "  Average: 49.34%\n",
      "\n",
      "======================================================================\n",
      "TASK 2\n",
      "======================================================================\n",
      "\n",
      "  Training...\n",
      "    E1/5 - Loss: 0.6872 (CE: 0.6870, EWC: 0.000003), Acc: 85.74%\n",
      "    E2/5 - Loss: 0.0124 (CE: 0.0123, EWC: 0.000004), Acc: 99.60%\n",
      "    E3/5 - Loss: 0.0056 (CE: 0.0054, EWC: 0.000004), Acc: 99.81%\n",
      "    E4/5 - Loss: 0.0025 (CE: 0.0023, EWC: 0.000004), Acc: 99.92%\n",
      "    E5/5 - Loss: 0.0015 (CE: 0.0013, EWC: 0.000004), Acc: 99.96%\n",
      "\n",
      "  Task 2 Accuracy:\n",
      "Accuracy on task 2: 99.95%\n",
      "\n",
      "  Computing Fisher information...\n",
      "    Fisher stats - Total: 0.000121, Norm: 0.0000000005\n",
      "\n",
      "  All Tasks (0-2):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.05%\n",
      "Accuracy on task 2: 99.95%\n",
      "  Average: 33.33%\n",
      "\n",
      "======================================================================\n",
      "TASK 3\n",
      "======================================================================\n",
      "\n",
      "  Training...\n",
      "    E1/5 - Loss: 0.9015 (CE: 0.9011, EWC: 0.000008), Acc: 85.96%\n",
      "    E2/5 - Loss: 0.0029 (CE: 0.0025, EWC: 0.000007), Acc: 99.93%\n",
      "    E3/5 - Loss: 0.0012 (CE: 0.0009, EWC: 0.000006), Acc: 99.98%\n",
      "    E4/5 - Loss: 0.0010 (CE: 0.0007, EWC: 0.000005), Acc: 99.97%\n",
      "    E5/5 - Loss: 0.0008 (CE: 0.0006, EWC: 0.000004), Acc: 99.98%\n",
      "\n",
      "  Task 3 Accuracy:\n",
      "Accuracy on task 3: 99.85%\n",
      "\n",
      "  Computing Fisher information...\n",
      "    Fisher stats - Total: 0.000077, Norm: 0.0000000003\n",
      "\n",
      "  All Tasks (0-3):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 99.85%\n",
      "  Average: 24.96%\n",
      "\n",
      "======================================================================\n",
      "TASK 4\n",
      "======================================================================\n",
      "\n",
      "  Training...\n",
      "    E1/5 - Loss: 1.0840 (CE: 1.0837, EWC: 0.000007), Acc: 85.29%\n",
      "    E2/5 - Loss: 0.0316 (CE: 0.0312, EWC: 0.000006), Acc: 99.00%\n",
      "    E3/5 - Loss: 0.0195 (CE: 0.0192, EWC: 0.000006), Acc: 99.41%\n",
      "    E4/5 - Loss: 0.0156 (CE: 0.0153, EWC: 0.000006), Acc: 99.53%\n",
      "    E5/5 - Loss: 0.0100 (CE: 0.0097, EWC: 0.000006), Acc: 99.68%\n",
      "\n",
      "  Task 4 Accuracy:\n",
      "Accuracy on task 4: 99.14%\n",
      "\n",
      "  Computing Fisher information...\n",
      "    Fisher stats - Total: 0.001193, Norm: 0.0000000051\n",
      "\n",
      "  All Tasks (0-4):\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 99.14%\n",
      "  Average: 19.83%\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Accuracy on task 0: 0.00%\n",
      "Accuracy on task 1: 0.00%\n",
      "Accuracy on task 2: 0.00%\n",
      "Accuracy on task 3: 0.00%\n",
      "Accuracy on task 4: 99.14%\n",
      "Average Accuracy: 19.83%\n",
      "\n",
      "  Task 0: 0.00%\n",
      "  Task 1: 0.00%\n",
      "  Task 2: 0.00%\n",
      "  Task 3: 0.00%\n",
      "  Task 4: 99.14%\n",
      "======================================================================\n",
      "✓ Model saved\n"
     ]
    }
   ],
   "source": [
    "from model import network_mnist, test_taskwise, test, train_stream, test_stream\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "class EWC_Working:\n",
    "    \"\"\"\n",
    "    EWC that actually works for continual learning.\n",
    "    Key: Store parameters AFTER training, compute Fisher on held-out validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ewc_lambda):\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.saved_params = {}\n",
    "        self.importances = {}\n",
    "    \n",
    "    def penalty(self, model):\n",
    "        \"\"\"Compute EWC penalty: Σ_t Σ_i F_t,i * (θ_i - θ*_t,i)^2\"\"\"\n",
    "        if len(self.saved_params) == 0:\n",
    "            return torch.tensor(0, device=next(model.parameters()).device, dtype=torch.float32)\n",
    "        \n",
    "        loss = torch.tensor(0, device=next(model.parameters()).device, dtype=torch.float32)\n",
    "        \n",
    "        for task_id in self.saved_params.keys():\n",
    "            for n, p in model.named_parameters():\n",
    "                fisher = self.importances[task_id][n]\n",
    "                saved_p = self.saved_params[task_id][n]\n",
    "                loss += (fisher * (p - saved_p).pow(2)).sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_fisher_on_data(self, model, criterion, train_loader, device):\n",
    "        \"\"\"\n",
    "        Compute Fisher information on the TRAINING DATA.\n",
    "        Use all batches, NOT just first epoch.\n",
    "        \"\"\"\n",
    "        model.eval()  # Use eval mode to avoid randomness from dropout/batchnorm\n",
    "        \n",
    "        fisher = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            fisher[n] = torch.zeros_like(p, device=device)\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        # Use ALL data to compute Fisher (more stable)\n",
    "        for x, y, *_ in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            for n, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.pow(2)\n",
    "            \n",
    "            num_samples += x.size(0)\n",
    "        \n",
    "        # Normalize by number of samples\n",
    "        for n in fisher:\n",
    "            fisher[n] /= float(num_samples)\n",
    "        \n",
    "        total_fisher = sum(v.sum().item() for v in fisher.values())\n",
    "        print(f\"    Fisher stats - Total: {total_fisher:.6f}, Norm: {total_fisher/sum(v.numel() for v in fisher.values()):.10f}\")\n",
    "        \n",
    "        return fisher\n",
    "    \n",
    "    def after_training(self, model, fisher_dict, task_id):\n",
    "        \"\"\"Store Fisher and current parameters.\"\"\"\n",
    "        self.importances[task_id] = fisher_dict\n",
    "        self.saved_params[task_id] = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "\n",
    "\n",
    "# ==================== Main Training Loop ====================\n",
    "\n",
    "model = network_mnist(256, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Properly tuned hyperparameters\n",
    "ewc_lambda = 50  # Much lower - Fisher values are actually reasonable\n",
    "learning_rate = 0.001\n",
    "epochs_per_task = 5\n",
    "batch_size = 64\n",
    "\n",
    "ewc = EWC_Working(ewc_lambda)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EWC Training - Final Working Version\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Lambda: {ewc_lambda}\")\n",
    "print(f\"Key changes:\")\n",
    "print(f\"  1. Store params AFTER training (not before)\")\n",
    "print(f\"  2. Compute Fisher on all training data (not just first epoch)\")\n",
    "print(f\"  3. Use model.eval() for Fisher (stable gradients)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK {task}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    experience = train_stream[task]\n",
    "    train_loader = DataLoader(experience.dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train on this task\n",
    "    print(f\"\\n  Training...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs_per_task):\n",
    "        total_loss = 0\n",
    "        total_ce = 0\n",
    "        total_ewc = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for x, y, *_ in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            ce_loss = criterion(out, y)\n",
    "            ewc_penalty = ewc.penalty(model)\n",
    "            loss = ce_loss + ewc_lambda * ewc_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            total_ewc += ewc_penalty.item()\n",
    "            \n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_ce = total_ce / len(train_loader)\n",
    "        avg_ewc = total_ewc / len(train_loader)\n",
    "        acc = 100 * correct / total_samples\n",
    "        \n",
    "        print(f\"    E{epoch+1}/5 - Loss: {avg_loss:.4f} (CE: {avg_ce:.4f}, EWC: {avg_ewc:.6f}), Acc: {acc:.2f}%\")\n",
    "    \n",
    "    # Test current task\n",
    "    print(f\"\\n  Task {task} Accuracy:\")\n",
    "    test_taskwise(model, task, device)\n",
    "    \n",
    "    # NOW compute Fisher AFTER training, but on training data\n",
    "    print(f\"\\n  Computing Fisher information...\")\n",
    "    fisher = ewc.compute_fisher_on_data(model, criterion, train_loader, device)\n",
    "    \n",
    "    # Store parameters and Fisher\n",
    "    ewc.after_training(model, fisher, task)\n",
    "    \n",
    "    # Test all tasks\n",
    "    print(f\"\\n  All Tasks (0-{task}):\")\n",
    "    all_accs = []\n",
    "    for t in range(task + 1):\n",
    "        acc = test_taskwise(model, t, device)\n",
    "        all_accs.append(acc)\n",
    "    \n",
    "    avg = sum(all_accs) / len(all_accs)\n",
    "    print(f\"  Average: {avg:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "final_avg, final_list = test(model, device)\n",
    "print()\n",
    "for i, acc in enumerate(final_list):\n",
    "    print(f\"  Task {i}: {acc:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "torch.save({'model': model.state_dict()}, 'ewc_working.pth')\n",
    "print(\"✓ Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3552d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
